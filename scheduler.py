import time
import schedule
import json
import requests
import os
import subprocess  # âœ… æ–°å¢ï¼šç”¨ä¾†åŸ·è¡Œ Git æŒ‡ä»¤
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# ==========================================
# 1. è¨­å®šç›£æ§è—äººåå–®
# ==========================================
MY_ARTISTS = [
    "10CM", "(G)I-DLE", "A train to autumn", "ADORA", "ADYA", "aespa", "AKMU", "Apink", "ARIAZ", 
    "BABYMONSTER", "BADVILLAIN", "Baek A Yeon", "BBGIRLS", "Billlie", "BLACKPINK", "BOL4", 
    "Brave Girls", "BTS", "BVNDIT", "Choi Yoo jung", "Chung Ha", "CLASS : y", "CLC", "CSR", 
    "DAY6", "Dreamcatcher", "EL7Z UP", "Ellui", "Eunha", "EVERGLOW", "FAVORITE", "FIFTY FIFTY", 
    "fromis_9", "Geenius", "GFRIEND", "Girls Planet 999", "GOT the beat", "GREE", "IU", 
    "KyoungSeo", "Kyung Dasom", "LA LIMA", "LE SSERAFIM", "LEE CHAE YEON", "LEE HI", "LIGHTSUM", 
    "lilli lilli", "Lim Kim", "LIMELIGHT", "Limesoda", "Lisa", "LOONA", "LUNARSOLAR", "LUNCH", 
    "Mamamoo", "mimiirose", "Minnie", "Miyeon", "MOMOLAND", "Moonbyul", "MRCH", "NANA", "NAYEON", 
    "NewJeans", "NMIXX", "NND", "OH MY GIRL", "PIXY", "PLAYBACK", "PRODUCE 48", "Punch", 
    "PURPLE KISS", "Qeendom2", "QWER", "Red Velvet", "RESCENE", "Rocket Punch", "Rolling Quartz", 
    "RosÃ©", "Rothy", "Ryu Su Jeong", "Saebit", "SECRET NUMBER", "Seo Dahyun", "SEULGI", "Shaun", 
    "SinB", "siso", "Solar", "Somi", "SOOJIN", "Soyeon", "STAYC", "Suzy", "SWAN", "T-ara", 
    "TAEYEON", "TRI.BE", "tripleS", "TWICE", "TZUYU", "Umji", "VIVIZ", "Weeekly", "Weki Meki", 
    "Wendy", "Wheein", "WINTER", "WJSN", "Woo Yerin", "woo!ah!", "WSG Wannabe", "X1", "XG", 
    "Yein", "YENA", "Yerin", "YongYong", "YooA", "Younha", "Yuju", "Yunsae", "Yuqi"
]

# ==========================================
# 2. å…¶ä»–è¨­å®š
# ==========================================
DATA_FILE = "songs_data.json"
KEEP_DAYS = 90

NAME_MAPPING = {}

# ==========================================
# âœ… æ–°å¢ï¼šGit è‡ªå‹•ä¸Šå‚³å‡½å¼
# ==========================================
def upload_to_github():
    print("ğŸš€ æº–å‚™ä¸Šå‚³æ›´æ–°åˆ° GitHub...")
    try:
        # 1. åŠ å…¥æª”æ¡ˆ (åªåŠ å…¥ json æª”ï¼Œé¿å…å‹•åˆ°å…¶ä»–æ±è¥¿)
        subprocess.run(["git", "add", DATA_FILE], check=True)
        
        # 2. æäº¤è®Šæ›´ (Commit)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        message = f"Auto update data: {timestamp}"
        # æ³¨æ„ï¼šå¦‚æœæª”æ¡ˆæ²’è®ŠåŒ–ï¼Œcommit æœƒå ±éŒ¯ï¼Œæ‰€ä»¥é€™è£¡ç”¨ try å¿½ç•¥æ²’è®ŠåŒ–çš„æƒ…æ³
        subprocess.run(["git", "commit", "-m", message], check=True)
        
        # 3. æ¨é€ (Push)
        subprocess.run(["git", "push"], check=True)
        print("âœ… GitHub ä¸Šå‚³æˆåŠŸï¼")
        
    except subprocess.CalledProcessError as e:
        # å¦‚æœæ˜¯ commit å¤±æ•—ï¼ˆé€šå¸¸æ˜¯å› ç‚ºæ²’æœ‰æ–°è®Šæ›´ï¼‰ï¼Œæˆ‘å€‘ä¸ç•¶ä½œéŒ¯èª¤
        if "nothing to commit" in str(e) or e.returncode == 1:
            print("ğŸ‘Œ æª”æ¡ˆç„¡è®Šæ›´ï¼Œè·³éä¸Šå‚³ã€‚")
        else:
            print(f"âŒ Git æ“ä½œå¤±æ•— (è«‹ç¢ºèªæœ‰è¨­å®šå…å¯†ç¢¼ç™»å…¥): {e}")
    except Exception as e:
        print(f"âŒ æœªçŸ¥éŒ¯èª¤: {e}")

def load_existing_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                if "songs" in data: return data["songs"]
                return []
        except:
            return []
    return []

def scrape_job():
    print(f"[{datetime.now()}] å•Ÿå‹•æ’ç¨‹ï¼šæª¢æŸ¥æ–°æ­Œèˆ‡æ¸…ç†èˆŠè³‡æ–™...")
    
    existing_songs = load_existing_data()
    existing_links = {song['link'] for song in existing_songs}
    new_songs = []
    
    try:
        url = "https://www.genie.co.kr/newest/song"
        headers = { "User-Agent": "Mozilla/5.0..." } # (çœç•¥é•·å­—ä¸²ä»¥ä¿æŒç°¡æ½”)
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        song_list = soup.select("table.list-wrap > tbody > tr")

        for song in song_list:
            try:
                artist_elem = song.select_one("a.artist")
                original_artist_name = artist_elem.text.strip() if artist_elem else "æœªçŸ¥è—äºº"

                is_target = False
                for target in MY_ARTISTS:
                    if target.lower() in original_artist_name.lower():
                        is_target = True
                        break
                
                if not is_target: continue 

                link_id = song['songid']
                link = f"https://www.genie.co.kr/detail/songInfo?xgnm={link_id}"

                if link in existing_links: continue

                display_artist_name = original_artist_name
                for key_word, custom_name in NAME_MAPPING.items():
                    if key_word.lower() in original_artist_name.lower():
                        display_artist_name = custom_name
                        break

                album_elem = song.select_one("a.albumtitle")
                title = album_elem.text.strip() if album_elem else "æœªçŸ¥å°ˆè¼¯"
                
                if "TITLE" in title: title = title.replace("TITLE", "").strip()
                if "19ê¸ˆ" in title: title = title.replace("19ê¸ˆ", "").strip()

                img_elem = song.select_one("a.cover img")
                img_src = "https:" + img_elem['src'] if img_elem else ""

                new_song = {
                    "artist": display_artist_name,
                    "title": title,
                    "image": img_src,
                    "link": link,
                    "found_at": datetime.now().strftime("%Y-%m-%d %H:%M")
                }
                
                new_songs.append(new_song)
                print(f"   -> ğŸ‰ ç™¼ç¾æ–°æ­Œï¼š{display_artist_name} - {title}")

            except Exception as e:
                continue

    except Exception as e:
        print(f"âš ï¸ çˆ¬èŸ²é€£ç·šå¤±æ•—: {e}")

    full_song_list = new_songs + existing_songs
    cutoff_date = datetime.now() - timedelta(days=KEEP_DAYS)
    final_list = []
    deleted_count = 0
    
    for song in full_song_list:
        try:
            song_date = datetime.strptime(song['found_at'], "%Y-%m-%d %H:%M")
            if song_date > cutoff_date:
                final_list.append(song)
            else:
                deleted_count += 1
        except ValueError:
            final_list.append(song)

    data_to_save = {
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "songs": final_list
    }
    
    try:
        # 1. å¯«å…¥æœ¬åœ°æª”æ¡ˆ
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=4)
            
        print(f"[{datetime.now()}] æœ¬åœ°å­˜æª”å®Œæˆ (æ–°å¢:{len(new_songs)}, åˆªé™¤:{deleted_count})")
        
        # 2. âœ… å‘¼å«ä¸Šå‚³ GitHub åŠŸèƒ½
        upload_to_github()
        
    except Exception as e:
        print(f"å­˜æª”å¤±æ•—: {e}")

if __name__ == "__main__":
    print(f"=== Genie çˆ¬èŸ²æ©Ÿå™¨äººå•Ÿå‹• (å« GitHub è‡ªå‹•åŒæ­¥) ===")
    
    scrape_job()
    
    print("å·²è¨­å®šæ’ç¨‹ï¼šæ¯å¤© 11:00, 17:00, 23:00 è‡ªå‹•æ›´æ–°")
    schedule.every().day.at("11:00").do(scrape_job)
    schedule.every().day.at("17:00").do(scrape_job)
    schedule.every().day.at("23:00").do(scrape_job)

    while True:
        schedule.run_pending()
        time.sleep(60)